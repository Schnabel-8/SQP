{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35fad365",
   "metadata": {},
   "source": [
    "Sequential Quadratic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221a9def",
   "metadata": {},
   "source": [
    "Dependencies: `jax`, `scipy`, `numpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cf415dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import jacfwd, jacrev\n",
    "\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab548147",
   "metadata": {},
   "source": [
    "Define your own Objective Functions and Constrains here :\n",
    "\n",
    "(we use rosenbrock function here as an example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ede887",
   "metadata": {},
   "source": [
    "$f(x)=\\sum^{N-1}_{i=1}100(x_{i+1}-{x_i}^2)^2+(1-x_i)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2fc1772",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the rosenbrock function\n",
    "def f(x):\n",
    "    return sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "443849e2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random\n",
    "\n",
    "key = random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3dd863e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jacfwd result, with shape (4, 3)\n",
      "[[ 0.05981758  0.12883787  0.08857603]\n",
      " [ 0.04015916 -0.04928625  0.00684531]\n",
      " [ 0.12188288  0.01406341 -0.3047072 ]\n",
      " [ 0.00140431 -0.00472531  0.00263782]]\n",
      "jacrev result, with shape (4, 3)\n",
      "[[ 0.05981757  0.12883787  0.08857603]\n",
      " [ 0.04015916 -0.04928625  0.00684531]\n",
      " [ 0.12188289  0.01406341 -0.3047072 ]\n",
      " [ 0.00140431 -0.00472531  0.00263782]]\n",
      "hessian, with shape (4, 3, 3)\n",
      "[[[ 0.02285465  0.04922541  0.03384247]\n",
      "  [ 0.04922541  0.10602397  0.07289147]\n",
      "  [ 0.03384247  0.07289147  0.05011288]]\n",
      "\n",
      " [[-0.03195215  0.03921401 -0.00544639]\n",
      "  [ 0.03921401 -0.04812629  0.00668421]\n",
      "  [-0.00544639  0.00668421 -0.00092836]]\n",
      "\n",
      " [[-0.01583708 -0.00182736  0.03959271]\n",
      "  [-0.00182736 -0.00021085  0.00456839]\n",
      "  [ 0.03959271  0.00456839 -0.09898177]]\n",
      "\n",
      " [[-0.00103524  0.00348343 -0.00194457]\n",
      "  [ 0.00348343 -0.01172127  0.0065432 ]\n",
      "  [-0.00194457  0.0065432  -0.00365263]]]\n",
      "W:  [-0.36838785 -2.275689    0.01144757]\n"
     ]
    }
   ],
   "source": [
    "from jax import jacfwd, jacrev\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 0.5 * (jnp.tanh(x / 2) + 1)\n",
    "\n",
    "# Outputs probability of a label being true.\n",
    "def predict(W, b, inputs):\n",
    "    return sigmoid(jnp.dot(inputs, W) + b)\n",
    "\n",
    "# Build a toy dataset.\n",
    "inputs = jnp.array([[0.52, 1.12,  0.77],\n",
    "                   [0.88, -1.08, 0.15],\n",
    "                   [0.52, 0.06, -1.30],\n",
    "                   [0.74, -2.49, 1.39]])\n",
    "targets = jnp.array([True, True, False, True])\n",
    "\n",
    "# Training loss is the negative log-likelihood of the training examples.\n",
    "def loss(W, b):\n",
    "    preds = predict(W, b, inputs)\n",
    "    label_probs = preds * targets + (1 - preds) * (1 - targets)\n",
    "    return -jnp.sum(jnp.log(label_probs))\n",
    "\n",
    "# Initialize random model coefficients\n",
    "key, W_key, b_key = random.split(key, 3)\n",
    "W = random.normal(W_key, (3,))\n",
    "b = random.normal(b_key, ())\n",
    "\n",
    "\n",
    "# Isolate the function from the weight matrix to the predictions\n",
    "f = lambda W: predict(W, b, inputs)\n",
    "\n",
    "J = jacfwd(f)(W)\n",
    "print(\"jacfwd result, with shape\", J.shape)\n",
    "print(J)\n",
    "\n",
    "J = jacrev(f)(W)\n",
    "print(\"jacrev result, with shape\", J.shape)\n",
    "print(J)\n",
    "\n",
    "def hessian(f):\n",
    "    return jacfwd(jacrev(f))\n",
    "\n",
    "H = hessian(f)(W)\n",
    "print(\"hessian, with shape\", H.shape)\n",
    "print(H)\n",
    "\n",
    "print(\"W: \",W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56bd0422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.0\n",
      "jacobian, with shape (3,)\n",
      "[ 3. 12. 27.]\n",
      "hessian, with shape (3, 3)\n",
      "[[ 6.  0.  0.]\n",
      " [ 0. 12.  0.]\n",
      " [ 0.  0. 18.]]\n"
     ]
    }
   ],
   "source": [
    "from jax import jacfwd, jacrev\n",
    "\n",
    "def f(x):\n",
    "    return jnp.power(x,3).sum()\n",
    "print(f(jnp.array([1.,2.,3.])))\n",
    "\n",
    "def hessian(f):\n",
    "    return jacfwd(jacrev(f))\n",
    "\n",
    "def jacobian(f):\n",
    "    return jacfwd(f)\n",
    "\n",
    "J = jacobian(f)(jnp.array([1.,2.,3.]))\n",
    "print(\"jacobian, with shape\", J.shape)\n",
    "print(J)\n",
    "\n",
    "H = hessian(f)(jnp.array([1.,2.,3.]))\n",
    "print(\"hessian, with shape\", H.shape)\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa7a5164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:  [[2. 1.]]\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.34271757499371747\n",
      "            Iterations: 4\n",
      "            Function evaluations: 5\n",
      "            Gradient evaluations: 4\n",
      "     fun: 0.34271757499371747\n",
      "     jac: array([-0.82676458, -0.41372478])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 5\n",
      "     nit: 4\n",
      "    njev: 4\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.41494475, 0.1701105 ])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "from scipy.optimize import Bounds\n",
    "\n",
    "bounds = Bounds([0, -0.5], [1.0, 2.0])\n",
    "\n",
    "def rosen(x):\n",
    "\n",
    "    \"\"\"The Rosenbrock function\"\"\"\n",
    "\n",
    "    return sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)\n",
    "\n",
    "def hessian(f):\n",
    "    return jacfwd(jacrev(f))\n",
    "\n",
    "def jacobian(f):\n",
    "    return jacfwd(f)\n",
    "\n",
    "rosen_der=jacobian(rosen)\n",
    "\n",
    "\n",
    "ineq_cons = {'type': 'ineq',\n",
    "\n",
    "             'fun' : lambda x: np.array([1 - x[0] - 2*x[1],\n",
    "\n",
    "                                         1 - x[0]**2 - x[1],\n",
    "\n",
    "                                         1 - x[0]**2 + x[1]]),\n",
    "\n",
    "             'jac' : lambda x: np.array([[-1.0, -2.0],\n",
    "\n",
    "                                         [-2*x[0], -1.0],\n",
    "\n",
    "                                         [-2*x[0], 1.0]])}\n",
    "\n",
    "def eqf(x):\n",
    "    return jnp.array([2*x[0] + x[1] - 1])\n",
    "    \n",
    "def jeq(x):\n",
    "    return np.array(jacobian(eqf)(x))\n",
    "    \n",
    "\n",
    "eq_cons = {'type': 'eq',\n",
    "\n",
    "           'fun' : lambda x: np.array([2*x[0] + x[1] - 1]),\n",
    "\n",
    "           #'jac' : lambda x: np.array([2.0, 1.0])}\n",
    "           'jac' : lambda x: np.array(jeq(x))}\n",
    "\n",
    "x0 = np.array([0.5, 0])\n",
    "print(\"1: \",jeq(x0))\n",
    "res = minimize(rosen, x0, method='SLSQP', jac=rosen_der,\n",
    "\n",
    "              constraints=[eq_cons,ineq_cons], options={'ftol': 1e-9, 'disp': True})\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c9feb15",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.41494432 0.17011135] -0.41348410536118446 2.4584779439852858e-11\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def rosen(x):\n",
    "\n",
    "    \"\"\"The Rosenbrock function\"\"\"\n",
    "\n",
    "    return sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)\n",
    "\n",
    "def hessian(f):\n",
    "    return jacfwd(jacrev(f))\n",
    "\n",
    "def jacobian(f):\n",
    "    return jacfwd(f)\n",
    "\n",
    "df=jacobian(rosen)\n",
    "\n",
    "def c(x):\n",
    "    return 2*x[0]+x[1]-1\n",
    "\n",
    "A=jacobian(c)\n",
    "C=hessian(c)\n",
    "W=hessian(rosen)\n",
    "\n",
    "x0 = np.array([0.5, 0])\n",
    "\n",
    "def phi(x,lamda):\n",
    "    return np.linalg.norm((df(x)-A(x)*lamda),ord=2)**2+c(x)**2\n",
    "beta=0.8\n",
    "epsi=1e-9\n",
    "xk=x0\n",
    "lamdak=1\n",
    "\n",
    "def left(x,lamda):\n",
    "    return np.vstack((np.hstack((W(x)-lamda*C(x),np.reshape(-A(x),(2,1)))),np.hstack((np.reshape(-A(x),(1,2)),np.reshape([0],(1,1))))))\n",
    "\n",
    "def right(x,lamda):\n",
    "    return -np.vstack((np.reshape(df(x),(2,1))-lamda*np.reshape(A(x),(2,1)),-np.reshape(c(x),(1,1))))\n",
    "\n",
    "while phi(xk,lamdak)>epsi:\n",
    "    dx=np.reshape(np.linalg.solve(left(xk,lamdak),right(xk,lamdak)),(3))\n",
    "    alpha=1\n",
    "    xkk=xk+alpha*np.array([dx[0],dx[1]])\n",
    "    lamdakk=lamdak+alpha*dx[2]\n",
    "    while(phi(xkk,lamdakk)>(1-beta*alpha)*phi(xk,lamdak)):\n",
    "        alpha=alpha/4\n",
    "        xkk=xk+alpha*np.array([dx[0],dx[1]])\n",
    "        lamdakk=lamdak+alpha*dx[2]\n",
    "    xk=xkk\n",
    "    lamdak=lamdakk\n",
    "#dx=np.reshape(np.linalg.solve(left(xk,lamdak),right(xk,lamdak)),(3))\n",
    "#print(np.reshape(dx,(3)))\n",
    "#print(xk+alpha*np.array([dx[0],dx[1]]))\n",
    "print(xk,lamdak,phi(xk,lamdak))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c23cbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "W=jax.hessian(f)\n",
    "g=jacfwd(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c131eb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqp_ker(f,eq,ineq,x0,epsi=1e-9,sigma=1,rho=0.8):\n",
    "    W=jax.hessian(f)\n",
    "    g=jacfwd(f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "886fb031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`gtol` termination condition is satisfied.\n",
      "Number of iterations: 10, function evaluations: 7, CG iterations: 6, optimality: 5.27e-09, constraint violation: 0.00e+00, execution time: 0.063 s.\n",
      " barrier_parameter: 0.0008000000000000003\n",
      " barrier_tolerance: 0.0008000000000000003\n",
      "          cg_niter: 6\n",
      "      cg_stop_cond: 1\n",
      "            constr: [array([0.75515031, 1.        ]), array([0.34228362, 0.00208321])]\n",
      "       constr_nfev: [0, 7]\n",
      "       constr_nhev: [0, 11]\n",
      "       constr_njev: [0, 7]\n",
      "    constr_penalty: 1.0\n",
      "  constr_violation: 0.0\n",
      "    execution_time: 0.06307411193847656\n",
      "               fun: 0.34271759984762334\n",
      "              grad: array([-0.8243288 , -0.41664243], dtype=float32)\n",
      "               jac: [array([[1., 2.],\n",
      "       [2., 1.]]), array([[ 0.8298998,  1.       ],\n",
      "       [ 0.8298998, -1.       ]])]\n",
      "   lagrangian_grad: array([-2.6501596e-09,  5.2692299e-09], dtype=float32)\n",
      "           message: '`gtol` termination condition is satisfied.'\n",
      "            method: 'tr_interior_point'\n",
      "              nfev: 7\n",
      "              nhev: 7\n",
      "               nit: 10\n",
      "             niter: 10\n",
      "              njev: 7\n",
      "        optimality: 5.26923e-09\n",
      "            status: 1\n",
      "           success: True\n",
      "         tr_radius: 701.877713212484\n",
      "                 v: [array([0.00326718, 0.40969343]), array([0.00121632, 0.00080168])]\n",
      "                 x: array([0.4149499, 0.1701002])\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import Bounds\n",
    "\n",
    "bounds = Bounds([0, -0.5], [1.0, 2.0])\n",
    "def rosen(x):\n",
    "\n",
    "    \"\"\"The Rosenbrock function\"\"\"\n",
    "\n",
    "    return sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)\n",
    "def rosen_hess(x):\n",
    "\n",
    "    x = np.asarray(x)\n",
    "\n",
    "    H = np.diag(-400*x[:-1],1) - np.diag(400*x[:-1],-1)\n",
    "\n",
    "    diagonal = np.zeros_like(x)\n",
    "\n",
    "    diagonal[0] = 1200*x[0]**2-400*x[1]+2\n",
    "\n",
    "    diagonal[-1] = 200\n",
    "\n",
    "    diagonal[1:-1] = 202 + 1200*x[1:-1]**2 - 400*x[2:]\n",
    "\n",
    "    H = H + np.diag(diagonal)\n",
    "\n",
    "    return H\n",
    "\n",
    "from scipy.optimize import LinearConstraint\n",
    "\n",
    "linear_constraint = LinearConstraint([[1, 2], [2, 1]], [-np.inf, 1], [1, 1])\n",
    "\n",
    "def cons_f(x):\n",
    "\n",
    "    return [x[0]**2 + x[1], x[0]**2 - x[1]]\n",
    "\n",
    "def cons_J(x):\n",
    "\n",
    "    return [[2*x[0], 1], [2*x[0], -1]]\n",
    "\n",
    "def cons_H(x, v):\n",
    "\n",
    "    return v[0]*np.array([[2, 0], [0, 0]]) + v[1]*np.array([[2, 0], [0, 0]])\n",
    "\n",
    "from scipy.optimize import NonlinearConstraint\n",
    "\n",
    "nonlinear_constraint = NonlinearConstraint(cons_f, -np.inf, 1, jac=cons_J, hess=cons_H)\n",
    "\n",
    "x0 = np.array([0.5, 0])\n",
    "\n",
    "res = minimize(rosen, x0, method='trust-constr', jac=rosen_der, hess=rosen_hess,\n",
    "\n",
    "               constraints=[linear_constraint, nonlinear_constraint],\n",
    "\n",
    "               options={'verbose': 1})\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d86f6dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
