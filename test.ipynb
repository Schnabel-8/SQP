{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35fad365",
   "metadata": {},
   "source": [
    "Sequential Quadratic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221a9def",
   "metadata": {},
   "source": [
    "Dependencies: `jax`, `scipy`, `numpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cf415dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jacfwd, jacrev, hessian\n",
    "\n",
    "from scipy.optimize import minimize, NonlinearConstraint\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab548147",
   "metadata": {},
   "source": [
    "Define your own Objective Functions and Constrains here :\n",
    "\n",
    "(we use rosenbrock function here as an example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ede887",
   "metadata": {},
   "source": [
    "$f(x)=\\sum^{N-1}_{i=1}100(x_{i+1}-{x_i}^2)^2+(1-x_i)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2fc1772",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the rosenbrock function\n",
    "def f(x):\n",
    "    return sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "443849e2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random\n",
    "\n",
    "key = random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3dd863e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jacfwd result, with shape (4, 3)\n",
      "[[ 0.05981758  0.12883787  0.08857603]\n",
      " [ 0.04015916 -0.04928625  0.00684531]\n",
      " [ 0.12188288  0.01406341 -0.3047072 ]\n",
      " [ 0.00140431 -0.00472531  0.00263782]]\n",
      "jacrev result, with shape (4, 3)\n",
      "[[ 0.05981757  0.12883787  0.08857603]\n",
      " [ 0.04015916 -0.04928625  0.00684531]\n",
      " [ 0.12188289  0.01406341 -0.3047072 ]\n",
      " [ 0.00140431 -0.00472531  0.00263782]]\n",
      "hessian, with shape (4, 3, 3)\n",
      "[[[ 0.02285465  0.04922541  0.03384247]\n",
      "  [ 0.04922541  0.10602397  0.07289147]\n",
      "  [ 0.03384247  0.07289147  0.05011288]]\n",
      "\n",
      " [[-0.03195215  0.03921401 -0.00544639]\n",
      "  [ 0.03921401 -0.04812629  0.00668421]\n",
      "  [-0.00544639  0.00668421 -0.00092836]]\n",
      "\n",
      " [[-0.01583708 -0.00182736  0.03959271]\n",
      "  [-0.00182736 -0.00021085  0.00456839]\n",
      "  [ 0.03959271  0.00456839 -0.09898177]]\n",
      "\n",
      " [[-0.00103524  0.00348343 -0.00194457]\n",
      "  [ 0.00348343 -0.01172127  0.0065432 ]\n",
      "  [-0.00194457  0.0065432  -0.00365263]]]\n",
      "W:  [-0.36838785 -2.275689    0.01144757]\n"
     ]
    }
   ],
   "source": [
    "from jax import jacfwd, jacrev\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 0.5 * (jnp.tanh(x / 2) + 1)\n",
    "\n",
    "# Outputs probability of a label being true.\n",
    "def predict(W, b, inputs):\n",
    "    return sigmoid(jnp.dot(inputs, W) + b)\n",
    "\n",
    "# Build a toy dataset.\n",
    "inputs = jnp.array([[0.52, 1.12,  0.77],\n",
    "                   [0.88, -1.08, 0.15],\n",
    "                   [0.52, 0.06, -1.30],\n",
    "                   [0.74, -2.49, 1.39]])\n",
    "targets = jnp.array([True, True, False, True])\n",
    "\n",
    "# Training loss is the negative log-likelihood of the training examples.\n",
    "def loss(W, b):\n",
    "    preds = predict(W, b, inputs)\n",
    "    label_probs = preds * targets + (1 - preds) * (1 - targets)\n",
    "    return -jnp.sum(jnp.log(label_probs))\n",
    "\n",
    "# Initialize random model coefficients\n",
    "key, W_key, b_key = random.split(key, 3)\n",
    "W = random.normal(W_key, (3,))\n",
    "b = random.normal(b_key, ())\n",
    "\n",
    "\n",
    "# Isolate the function from the weight matrix to the predictions\n",
    "f = lambda W: predict(W, b, inputs)\n",
    "\n",
    "J = jacfwd(f)(W)\n",
    "print(\"jacfwd result, with shape\", J.shape)\n",
    "print(J)\n",
    "\n",
    "J = jacrev(f)(W)\n",
    "print(\"jacrev result, with shape\", J.shape)\n",
    "print(J)\n",
    "\n",
    "def hessian(f):\n",
    "    return jacfwd(jacrev(f))\n",
    "\n",
    "H = hessian(f)(W)\n",
    "print(\"hessian, with shape\", H.shape)\n",
    "print(H)\n",
    "\n",
    "print(\"W: \",W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56bd0422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.0\n",
      "jacobian, with shape (3,)\n",
      "[ 3. 12. 27.]\n",
      "hessian, with shape (3, 3)\n",
      "[[ 6.  0.  0.]\n",
      " [ 0. 12.  0.]\n",
      " [ 0.  0. 18.]]\n"
     ]
    }
   ],
   "source": [
    "from jax import jacfwd, jacrev\n",
    "\n",
    "def f(x):\n",
    "    return jnp.power(x,3).sum()\n",
    "print(f(jnp.array([1.,2.,3.])))\n",
    "\n",
    "def hessian(f):\n",
    "    return jacfwd(jacrev(f))\n",
    "\n",
    "def jacobian(f):\n",
    "    return jacfwd(f)\n",
    "\n",
    "J = jacobian(f)(jnp.array([1.,2.,3.]))\n",
    "print(\"jacobian, with shape\", J.shape)\n",
    "print(J)\n",
    "\n",
    "H = hessian(f)(jnp.array([1.,2.,3.]))\n",
    "print(\"hessian, with shape\", H.shape)\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa7a5164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:  [[2. 1.]]\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3427175748433131\n",
      "            Iterations: 5\n",
      "            Function evaluations: 9\n",
      "            Gradient evaluations: 5\n",
      "     fun: 0.3427175748433131\n",
      "     jac: array([-0.82696629, -0.41348338])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 9\n",
      "     nit: 5\n",
      "    njev: 5\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.41494432, 0.17011137])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "from scipy.optimize import Bounds\n",
    "\n",
    "bounds = Bounds([0, -0.5], [1.0, 2.0])\n",
    "\n",
    "def rosen(x):\n",
    "\n",
    "    \"\"\"The Rosenbrock function\"\"\"\n",
    "\n",
    "    return sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)\n",
    "\n",
    "def hessian(f):\n",
    "    return jacfwd(jacrev(f))\n",
    "\n",
    "def jacobian(f):\n",
    "    return jacfwd(f)\n",
    "\n",
    "rosen_der=jacobian(rosen)\n",
    "\n",
    "\n",
    "ineq_cons = {'type': 'ineq',\n",
    "\n",
    "             'fun' : lambda x: np.array([1 - x[0] - 2*x[1],\n",
    "\n",
    "                                         1 - x[0]**2 - x[1],\n",
    "\n",
    "                                         1 - x[0]**2 + x[1]]),\n",
    "\n",
    "             'jac' : lambda x: np.array([[-1.0, -2.0],\n",
    "\n",
    "                                         [-2*x[0], -1.0],\n",
    "\n",
    "                                         [-2*x[0], 1.0]])}\n",
    "\n",
    "def eqf(x):\n",
    "    return jnp.array([2*x[0] + x[1] - 1])\n",
    "    \n",
    "def jeq(x):\n",
    "    return np.array(jacobian(eqf)(x))\n",
    "    \n",
    "\n",
    "eq_cons = {'type': 'eq',\n",
    "\n",
    "           'fun' : lambda x: np.array([2*x[0] + x[1] - 1]),\n",
    "\n",
    "           #'jac' : lambda x: np.array([2.0, 1.0])}\n",
    "           'jac' : lambda x: np.array(jeq(x))}\n",
    "\n",
    "x0 = np.array([0.5, 0])\n",
    "print(\"1: \",jeq(x0))\n",
    "res = minimize(rosen, x0, method='SLSQP', jac=rosen_der,\n",
    "\n",
    "              constraints=[eq_cons], options={'ftol': 1e-9, 'disp': True})\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c9feb15",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.41494432 0.17011135] -0.41348410536118446 2.4584779439852858e-11\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def rosen(x):\n",
    "\n",
    "    \"\"\"The Rosenbrock function\"\"\"\n",
    "\n",
    "    return sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)\n",
    "\n",
    "def hessian(f):\n",
    "    return jacfwd(jacrev(f))\n",
    "\n",
    "def jacobian(f):\n",
    "    return jacfwd(f)\n",
    "\n",
    "df=jacobian(rosen)\n",
    "\n",
    "def c(x):\n",
    "    return 2*x[0]+x[1]-1\n",
    "\n",
    "A=jacobian(c)\n",
    "C=hessian(c)\n",
    "W=hessian(rosen)\n",
    "\n",
    "x0 = np.array([0.5, 0])\n",
    "\n",
    "def phi(x,lamda):\n",
    "    return np.linalg.norm((df(x)-A(x)*lamda),ord=2)**2+c(x)**2\n",
    "beta=0.8\n",
    "epsi=1e-9\n",
    "xk=x0\n",
    "lamdak=1\n",
    "\n",
    "def left(x,lamda):\n",
    "    return np.vstack((np.hstack((W(x)-lamda*C(x),np.reshape(-A(x),(2,1)))),np.hstack((np.reshape(-A(x),(1,2)),np.reshape([0],(1,1))))))\n",
    "\n",
    "def right(x,lamda):\n",
    "    return -np.vstack((np.reshape(df(x),(2,1))-lamda*np.reshape(A(x),(2,1)),-np.reshape(c(x),(1,1))))\n",
    "\n",
    "while phi(xk,lamdak)>epsi:\n",
    "    dx=np.reshape(np.linalg.solve(left(xk,lamdak),right(xk,lamdak)),(3))\n",
    "    alpha=1\n",
    "    xkk=xk+alpha*np.array([dx[0],dx[1]])\n",
    "    lamdakk=lamdak+alpha*dx[2]\n",
    "    while(phi(xkk,lamdakk)>(1-beta*alpha)*phi(xk,lamdak)):\n",
    "        alpha=alpha/4\n",
    "        xkk=xk+alpha*np.array([dx[0],dx[1]])\n",
    "        lamdakk=lamdak+alpha*dx[2]\n",
    "    xk=xkk\n",
    "    lamdak=lamdakk\n",
    "#dx=np.reshape(np.linalg.solve(left(xk,lamdak),right(xk,lamdak)),(3))\n",
    "#print(np.reshape(dx,(3)))\n",
    "#print(xk+alpha*np.array([dx[0],dx[1]]))\n",
    "print(xk,lamdak,phi(xk,lamdak))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c23cbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "W=jax.hessian(f)\n",
    "g=jacfwd(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c131eb44",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cons_f' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_45790/3211186515.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mxk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0malphak\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# test genW and cons_diff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0meq_jac_ker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meq_hess_ker\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstrain_diff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcons_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mineq_jac_ker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mineq_hess_ker\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstrain_diff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcons_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m#print(len(np.vstack((eq_hess_ker(x0),ineq_hess_ker(x0)))))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cons_f' is not defined"
     ]
    }
   ],
   "source": [
    "def constrain_diff(*args):\n",
    "        args=list(args)\n",
    "        cons_jac=lambda x: np.array(jax.jacfwd(args[0])(x))\n",
    "        cons_hess=lambda x:np.array(jax.hessian(args[0])(x))\n",
    "        return [cons_jac,cons_hess]\n",
    "def genW(f_W,cons_hess_W,x0_W):\n",
    "        len_hess_W=len(cons_hess_W(x0_W))\n",
    "        f_jac_W=lambda x:np.array(jax.hessian(f_W)(x))\n",
    "        def W(x_W,lmd_W):\n",
    "            W_=f_jac_W(x_W)\n",
    "            for i_W in range(len_hess_W):\n",
    "                W_-=lmd_W[i_W]*cons_hess_W(x_W)[i_W]\n",
    "            return W_\n",
    "        return W\n",
    "# d is a row vector\n",
    "# this function compute the objective of subproblem\n",
    "# for given W_k and g_k\n",
    "def obj(Wk,gk):\n",
    "    return lambda d:0.5*np.dot(d,np.dot(Wk,d.T))+np.dot(gk,d)\n",
    "# L1 penal function\n",
    "def P(f_P,sigma_P,eq_P,ineq_P):\n",
    "    return lambda x:f_P(x)+sigma_P*(np.sum(np.abs(eq_P[0](x)))+np.sum(np.maximum(ineq_P[0](x),0,out=None)))\n",
    "def gen_cons(cons_,cons_jac_,xk_):\n",
    "    return lambda d:cons_(xk_)+np.dot(xk_,d)\n",
    "def sqp_ker(f,eq,ineq,x0,epsi=1e-9,sigma=1,rho=0.8):\n",
    "    # diffrentiate the constrains\n",
    "    [eq_jac_ker,eq_hess_ker]=constrain_diff(eq)\n",
    "    [ineq_jac_ker,ineq_hess_ker]=constrain_diff(ineq)\n",
    "    # generate matrix W and vector g\n",
    "    W_ker=genW(f,lambda x: np.vstack((eq_hess_ker(x),ineq_hess_ker(x))),x0)\n",
    "    g_ker=lambda d: np.array(jacfwd(f)(d))\n",
    "    \n",
    "    itr=0\n",
    "    dk=np.inf\n",
    "    while(np.linalg.norm(dk,ord=2)>epsi or itr==0):\n",
    "        itr+=1\n",
    "        objk=lambda d:obj(d,W(xk,lamdak),g(xk))\n",
    "        jack=lambda d:np.array(jacfwd(objk)(d))\n",
    "        hessk=lambda d:np.array(jax.hessian(objk)(d))\n",
    "        dk0=dk\n",
    "        res = minimize(objk, dk0, method='trust-constr', jac=jack, hess=hessk,\n",
    "                      constraints=constr(eq,ineq),\n",
    "                      options={'verbose': 0})\n",
    "        alphak=1\n",
    "        one_dim_search(P,)\n",
    "        xk=xk+alphak*dk\n",
    "# test genW and cons_diff\n",
    "[eq_jac_ker,eq_hess_ker]=constrain_diff(cons_f, 1, 1)\n",
    "[ineq_jac_ker,ineq_hess_ker]=constrain_diff(cons_f, 1, 1)\n",
    "#print(len(np.vstack((eq_hess_ker(x0),ineq_hess_ker(x0)))))\n",
    "W_ker=genW(f,lambda x:np.vstack((eq_hess_ker(x),ineq_hess_ker(x))),x0)   \n",
    "#print(W_ker(x0,np.array([1,1,1,1])))\n",
    "g_ker=obj([[1,1],[2,2]],[1,1])(np.array([1,1]))\n",
    "#print([cons_f, 1, 1][0](np.array([1,1])))\n",
    "P_ker=P(rosen,1,[cons_f, 1, 1],[cons_f, 1, 1])\n",
    "print(P_ker(np.array([1,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886fb031",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import Bounds\n",
    "\n",
    "bounds = Bounds([0, -0.5], [1.0, 2.0])\n",
    "def rosen(x):\n",
    "\n",
    "    \"\"\"The Rosenbrock function\"\"\"\n",
    "\n",
    "    return sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)\n",
    "def rosen_hess(x):\n",
    "\n",
    "    return np.array(jax.hessian(rosen)(x))\n",
    "\n",
    "from scipy.optimize import LinearConstraint\n",
    "\n",
    "linear_constraint = LinearConstraint([2, 1], [1], [1])\n",
    "\n",
    "def cons_f(x):\n",
    "\n",
    "    return [x[0]**2 + x[1], x[0]**2 - x[1]]\n",
    "\n",
    "def cons_J(x):\n",
    "\n",
    "    return [[2*x[0], 1], [2*x[0], -1]]\n",
    "\n",
    "def cons_H(x, v):\n",
    "\n",
    "    return v[0]*np.array([[2, 0], [0, 0]]) + v[1]*np.array([[2, 0], [0, 0]])\n",
    "\n",
    "from scipy.optimize import NonlinearConstraint\n",
    "\n",
    "def func2(*args):\n",
    "    args=list(args)\n",
    "    length=len(jacfwd(args[0])(args[5]))\n",
    "    a=0*args[5]\n",
    "    for i in range(length):\n",
    "        print(i)\n",
    "        a=a+jacfwd(args[0])(args[5])[i]\n",
    "        \n",
    "    print(a)\n",
    "    print(jax.hessian(args[0])(args[5]))\n",
    "    args.pop()\n",
    "    return NonlinearConstraint(*args)\n",
    "cj=lambda x: np.array(jax.jacfwd(cons_f)(x))\n",
    "def ch(x,v):\n",
    "    hess=np.array(jax.hessian(cons_f)(x))\n",
    "    length=len(hess)\n",
    "    ret=0*hess[0]\n",
    "    for i in range(length):\n",
    "        ret=ret+v[i]*hess[i]\n",
    "    return ret\n",
    "#nonlinear_constraint = NonlinearConstraint(cons_f, 1, 1, jac=cons_J, hess=cons_H)\n",
    "\n",
    "nonlinear_constraint=func2(cons_f, 1, 1,cj,ch,x0)\n",
    "x0 = np.array([0.5, 0])\n",
    "\n",
    "res = minimize(rosen, x0, method='trust-constr', jac=rosen_der, hess=rosen_hess,\n",
    "\n",
    "               constraints=[nonlinear_constraint],\n",
    "\n",
    "               options={'verbose': 1})\n",
    "print(res.x)\n",
    "print(rosen_hess(x0))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a51ccc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter :   1 p:   0.7447199899674442 xk:   [0.50143112 0.19678437] dk:   0.2491007236018366 alphak:    0.79\n",
      "iter :   2 p:   0.8039492436807195 xk:   [0.50197016 0.23483961] dk:   0.052135703400866124 alphak:    0.73\n",
      "iter :   3 p:   0.8528351360222941 xk:   [0.50213773 0.24510352] dk:   0.014062017612101975 alphak:    0.73\n",
      "iter :   4 p:   0.8695361758604804 xk:   [0.50216198 0.247888  ] dk:   0.0037629567434796837 alphak:    0.74\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"cons=ineq\\nprint(cons(x0))\\nconsjac=lambda x:np.array(jacfwd(cons)(x))\\nprint(consjac(x0))\\nconshess=lambda x,v:np.sum(np.reshape(np.array(v),(4,1,1))*np.array(hessian(cons)(x)),axis=0)\\nprint(conshess(x0,np.array([1,1,1,1])))\\nnonlinear_constraint = NonlinearConstraint(lambda x:np.array(cons(x)), 0, np.inf, jac=consjac, hess=conshess)\\nres=minimize(f,x0,method='trust-constr',jac=jac_f,hess=hess_f,\\n                    constraints=[nonlinear_constraint],options={'verbose': 1})\\nprint(res.x)\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jacfwd, jacrev, hessian\n",
    "\n",
    "from scipy.optimize import minimize, NonlinearConstraint\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "x0=np.array([0.5,0.])\n",
    "def f(x):\n",
    "    #return -jnp.sin(x[0])*jnp.cos(x[1])-jnp.sin(x[1])*jnp.cos(x[0])\n",
    "    return sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)\n",
    "    \n",
    "jac_f=lambda x:np.array(jacfwd(f)(x))\n",
    "hess_f=lambda x:np.array(hessian(f)(x))\n",
    "\n",
    "def ineq(x):\n",
    "    #return [x[0]+x[1],-x[0]-x[1]-jnp.exp(-7*x[0])+jnp.pi,x[0]-x[1]**3,x[1]**3-x[0]]\n",
    "    return [1-x[0]-2*x[1],1-x[0]**2-x[1],1-x[0]**2+x[1]]\n",
    "jac_ineq=lambda x:np.array(jacfwd(ineq)(x))\n",
    "hess_ineq=lambda x:np.array(hessian(ineq)(x))\n",
    "\n",
    "def genW(f_hess_W,cons_hess_W,x0_W):\n",
    "        len_W=len(cons_hess_W(x0_W))\n",
    "        return lambda x,lmd:hess_f(x)-np.sum(np.reshape(lmd,(len_W,1,1))*np.array(hess_ineq(x0)),axis=0)\n",
    "# d is a row vector\n",
    "# this function compute the objective of subproblem\n",
    "# for given W_k and g_k\n",
    "# obj must use jnp instead of np\n",
    "def obj(Wk,gk):\n",
    "    return lambda d:0.5*jnp.dot(d,jnp.dot(Wk,d.T))+jnp.dot(gk,d)\n",
    "# L1 penal function\n",
    "def P(f_P,sigma_P,ineq_P):\n",
    "    return lambda x:f_P(x)+sigma_P*(np.sum(np.abs(np.minimum(ineq_P(x),0,out=None))))\n",
    "# gen_cons must use jnp instead of np\n",
    "def gen_cons(cons_,cons_jac_,xk_):\n",
    "    return lambda d:jnp.array(cons_(xk_))+jnp.dot(cons_jac_(xk_),d)\n",
    "\n",
    "def sqp_test(x0,epsi=1e-3,sigma=0.5,rho=0.8):\n",
    "    k=0\n",
    "    xk=x0\n",
    "    lmdk=np.array([0,0,0])\n",
    "    # caculate Wk\n",
    "    Wker=genW(hess_f,hess_ineq,x0)\n",
    "    # caculate gk\n",
    "    gker=lambda x: np.array(jacfwd(f)(x))\n",
    "    \n",
    "    alpha_step=0.01\n",
    "    alpha=np.arange(0,rho,alpha_step)\n",
    "    plist=alpha\n",
    "    \n",
    "    iter=0\n",
    "    \n",
    "    while(True):\n",
    "        Wk=Wker(xk,lmdk)\n",
    "        gk=gker(xk)\n",
    "        cons=gen_cons(ineq,jac_ineq,xk)\n",
    "        len_cons=len(cons(x0))\n",
    "        consjac=lambda x:np.array(jacfwd(cons)(x))\n",
    "        conshess=lambda x,v:np.sum(np.reshape(np.array(v),(len_cons,1,1))*np.array(hessian(cons)(x)),axis=0)\n",
    "        nonlinear_constraint = NonlinearConstraint(lambda x:np.array(cons(x)), 0, np.inf, jac=consjac, hess=conshess)\n",
    "        obj_=obj(Wk,gk)\n",
    "        objjac=lambda x:np.array(jacfwd(obj_)(x))\n",
    "        objhess=lambda x:np.array(hessian(obj_)(x))\n",
    "        res=minimize(obj_,x0,method='trust-constr',jac=objjac,hess=objhess,\n",
    "                    constraints=[nonlinear_constraint],options={'verbose': 0})\n",
    "        lmdk=np.reshape(np.array(res.v),-1)\n",
    "        #print(np.reshape(lmdk,-1)\n",
    "        dk=np.array(res.x)\n",
    "        if(np.linalg.norm(dk,ord=2)<=epsi):\n",
    "            break;\n",
    "            \n",
    "        pfunc=P(f,sigma,cons)\n",
    "        count=0\n",
    "        for alphak in alpha:\n",
    "            plist[count]=pfunc(xk+alphak*dk)\n",
    "            count+=1\n",
    "        alphak=alpha_step*np.argmin(plist)\n",
    "        xk=xk+alphak*dk\n",
    "        iter+=1\n",
    "        print(\"iter :  \",iter,\"p:  \",pfunc(xk),\"xk:  \",xk,\"dk:  \",np.linalg.norm(dk,ord=2),\"alphak:   \",alphak)\n",
    "    return xk\n",
    "sqp_test(x0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8c05e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.24889703505929078\n",
      "            Iterations: 15\n",
      "            Function evaluations: 23\n",
      "            Gradient evaluations: 15\n",
      "[0.50220282 0.24889859]\n"
     ]
    }
   ],
   "source": [
    "def rosen(x):\n",
    "\n",
    "    \"\"\"The Rosenbrock function\"\"\"\n",
    "\n",
    "    return sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)\n",
    "\n",
    "def rosen_der(x):\n",
    "\n",
    "    xm = x[1:-1]\n",
    "\n",
    "    xm_m1 = x[:-2]\n",
    "\n",
    "    xm_p1 = x[2:]\n",
    "\n",
    "    der = np.zeros_like(x)\n",
    "\n",
    "    der[1:-1] = 200*(xm-xm_m1**2) - 400*(xm_p1 - xm**2)*xm - 2*(1-xm)\n",
    "\n",
    "    der[0] = -400*x[0]*(x[1]-x[0]**2) - 2*(1-x[0])\n",
    "\n",
    "    der[-1] = 200*(x[-1]-x[-2]**2)\n",
    "\n",
    "    return der\n",
    "\n",
    "ineq_cons = {'type': 'ineq',\n",
    "\n",
    "             'fun' : lambda x: np.array([1 - x[0] - 2*x[1],\n",
    "\n",
    "                                         1 - x[0]**2 - x[1],\n",
    "\n",
    "                                         1 - x[0]**2 + x[1]]),\n",
    "\n",
    "             'jac' : lambda x: np.array([[-1.0, -2.0],\n",
    "\n",
    "                                         [-2*x[0], -1.0],\n",
    "\n",
    "                                         [-2*x[0], 1.0]])}\n",
    "\n",
    "x0 = np.array([0.5, 0])\n",
    "\n",
    "res = minimize(rosen, x0, method='SLSQP', jac=rosen_der,\n",
    "\n",
    "               constraints=[ ineq_cons], options={'ftol': 1e-9, 'disp': True})\n",
    "print(res.x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1930a7cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
